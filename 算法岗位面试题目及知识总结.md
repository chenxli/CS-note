## 算法岗位面试题目及知识总结

### <u>C++</u>

1. c++中指针和引用的区别（2020字节跳动）

   * 指针：指针是一个变量，只不过这个变量存储的是一个地址，指向内存的一个存储单元；而引用跟原来的变量实质上是同一个东西，只不过是原变量的一个别名而已
   * 可以有const指针，但是没有const引用
   * 指针可以有多级，但是引用只能是一级
   * 指针的值可以为空，但是引用的值不能为NULL，并且引用在定义的时候必须初始化
   * 指针的值在初始化后可以改变，即指向其它的存储单元，而引用在进行初始化后就不会再改变了（绑定在一起）
   * "sizeof引用"得到的是所指向的变量(对象)的大小，而"sizeof指针"得到的是指针本身的大小
   * 指针和引用的自增(++)运算意义不一样

2. 如果实现c++中的vector，只需push_back和查找两个功能，底层如何实现（2020字节跳动）

   解答：用一个数组，不够就定义一个更大的，复制过去；查找可以顺序查找

   ​           更快的方法：二叉查找树

### <u>操作系统</u>

1. 如何从用户态进入内核态(2020字节跳动)
2. 两个很大的文件(一次读不进来的那种)，想找两文件相同的URL，怎么实现

### <u>算法题</u>

1. 两个单链表找到第一个公共节点（2020字节跳动）

   解法：求两个链表长度， 长的先走K步，然后再一起走

2. 由0和1组成的二维矩阵，找出1的最大连通域，计算其面积（2020字节跳动）

   解法：DFS

3. 长度为n的字符串中包含m个不同的字符，找出包含这m个不同字符的最小子串（2020字节跳动）

4. 长度为n的数组中有一个数字出现了n/2次，快速找到这个数。（2020字节跳动）

5. n个人之间存在m个关系对，关系具有传递性，假如A关注B，B关注C，那么A就间接关注了C。如果一个人被除他之外的所有人都直接或间接关注，那么这个人就是抖音红人，求抖音红人的总数。（2020字节跳动）

   解法：并查集（传递性、朋友的朋友也是朋友）

6. 一颗二叉树，找到最长路径，该路径中的所有节点值都相等，该路径可以不包含根结点，路径的长度是边的数量（2020TP-LINK）

7. 排序算法有多少了解，手写快排序。

8. n叉树的之字形遍历。

9. 单链表排序,要求速度要快，不可以转列表。

10. 合并链表奇数位升序，偶数位降序。(原题)

### <u>机器学习</u>

1. xgb、lgb、catboost区别

   |      | xgb  | lgb  | cat  |
   | ---- | ---- | ---- | ---- |
   | 优点 |      |      |      |
   | 缺点 |      |      |      |

2. GDBT原理和随机森林等算法比较（2020字节跳动)

3. SVM损失函数推导（2020字节跳动）

4. 朴素贝叶斯写公式（2020字节跳动）

5. xgboost和GBDT的分裂方式哪个好（2020字节跳动）

6. 介绍一下xgboost有哪些特点（2020字节跳动）

7. 非线性分类算法有哪些（2020字节跳动）

8. xgboost相比传统gbdt有何不同？xgboost为什么快？xgboost如何支持并行？

   xgb不仅支持决策树作为基分类器，还支持线性分类器。

   用到了loss函数的二阶泰勒展开，因此与损失函数的更接近，收敛更快。

   在代价函数中加入了正则项，用于控制模型复杂度。正则项里包括了树的叶子节点个数和叶子结点输出值的L2范数，可以防止模型过拟合。

   Shrinkage，学习速率，主要用于削弱每课树的影响，让后面有更大的学习空间，实际应用中，一般把它设小点，迭代次数设大点。

   列抽样，xgboost从随机森林算法中借鉴来的，支持列抽样可以降低过拟合，并且减少计算。

   支持对缺失值的处理，对于特征值缺失的样本，xgboost可以学习这些缺失值的分裂方向。

   支持并行。在每棵树进行节点分裂时，需要计算每个特征的增益，选择最大的那个特征作为分裂特征，各个特征的增益计算可以开多线程进行。这是因为xgboost对于每列数据，存储在一个内存单元block结构里面，在分裂之前，就可以对其进行预排序以及记录统计数据，那么可以在每次迭代中重复使用。

   近似算法，树节点分裂时，需要枚举每个可能的分割点。当数据没法一次性载入内存时，这种方法会很慢，xgboost提出了一种近似的方法去高效的生成候选分割点。根据特征值的百分位数提出候选分裂节点，然后把每个点分配到这些桶里面去。

9.  生成模型、判别模型

   生成模型：对联合概率分布进行建模。HMM、朴素贝叶斯、贝叶斯网络

   判别模型：直接对条件概率分布进行建模。线性回归模型、SVM、最大熵、条件随机场

10. 数据样本不平衡的解决办法？

    数据方面：

    - 扩大数据集

    - 数据集重采样：小类数据样本个数：过采样（采样的个数大于该类样本的个数）

      ​                           大类数据样本分数：欠采样（采样的次数少于该类样本的个数

    算法方面：

    - 尝试不同分类算法
    - 对小类错分进行加权惩罚

11. 最大似然估计和最大后验估计

    最大似然估计是求 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta) 使得似然函数 ![[公式]](https://www.zhihu.com/equation?tex=P%28x_0%7C%5Ctheta%29) 最大；

    最大后验概率估计是求 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta) 使得函数![[公式]](https://www.zhihu.com/equation?tex=P%28x_0%7C%5Ctheta%29+P%28%5Ctheta%29) 最大， ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta) 自己出现的先验概率也最大（其实就是考虑了参数的先验概率）；

12. auc的含义

    ROC曲线下的面积

13. 手推并描述EM算法

14.  LR的数据流中有一个重复会有什么影响

15. 特征选择的方法

    - 过滤法：通过计算各特征与标签之间的相关系数，特征方差，卡方检验，互信息等进行选择；

    - 包裹法：将特征分成多个子集，用一个模型分别训练每个特征子集，找到最好的一个子集；

      包装法的解决思路没有过滤法这么直接，它会选择一个目标函数来一步步的筛选特征。

      最常用的包装法是递归消除特征法(recursive feature elimination,以下简称RFE)。递归消除特征法使用一个机器学习模型来进行多轮训练，每轮训练后，消除若干权值系数的对应的特征，再基于新的特征集进行下一轮训练。在sklearn中，可以使用RFE函数来选择特征。

    - 嵌入法：使用自带特征选择的学习算法，比如LR。

      嵌入法也是用机器学习的方法来选择特征，但是它和RFE的区别是它不是通过不停的筛掉特征来进行训练，而是使用的都是特征全集。在sklearn中，使用SelectFromModel函数来选择特征。

      最常用的是使用L1正则化和L2正则化来选择特征。在之前讲到的[用scikit-learn和pandas学习Ridge回归](http://www.cnblogs.com/pinard/p/6023000.html)第6节中，我们讲到正则化惩罚项越大，那么模型的系数就会越小。当正则化惩罚项大到一定的程度的时候，部分特征系数会变成0，当正则化惩罚项继续增大到一定程度时，所有的特征系数都会趋于0. 但是我们会发现一部分特征系数会更容易先变成0，这部分系数就是可以筛掉的。也就是说，我们选择特征系数较大的特征。常用的L1正则化和L2正则化来选择特征的基学习器是逻辑回归。

      此外也可以使用决策树或者GBDT。那么是不是所有的机器学习方法都可以作为嵌入法的基学习器呢？也不是，一般来说，可以得到特征系数coef或者可以得到特征重要度(feature importances)的算法才可以做为嵌入法的基学习器。

    - 寻找高级特征

      在我们拿到已有的特征后，我们还可以根据需要寻找到更多的高级特征。比如有车的路程特征和时间间隔特征，我们就可以得到车的平均速度这个二级特征。根据车的速度特征，我们就可以得到车的加速度这个三级特征，根据车的加速度特征，我们就可以得到车的加加速度这个四级特征。。。也就是说，高级特征可以一直寻找下去。

      在Kaggle之类的算法竞赛中，高分团队主要使用的方法除了集成学习算法，剩下的主要就是在高级特征上面做文章。所以寻找高级特征是模型优化的必要步骤之一。当然，在第一次建立模型的时候，我们可以先不寻找高级特征，得到以后基准模型后，再寻找高级特征进行优化。

      寻找高级特征最常用的方法有：

      若干项特征加和： 我们假设你希望根据每日销售额得到一周销售额的特征。你可以将最近的7天的销售额相加得到。
      若干项特征之差： 假设你已经拥有每周销售额以及每月销售额两项特征，可以求一周前一月内的销售额。
      若干项特征乘积： 假设你有商品价格和商品销量的特征，那么就可以得到销售额的特征。
      若干项特征除商： 假设你有每个用户的销售额和购买的商品件数，那么就是得到该用户平均每件商品的销售额。

      当然，寻找高级特征的方法远不止于此，它需要你根据你的业务和模型需要而得，而不是随便的两两组合形成高级特征，这样容易导致特征爆炸，反而没有办法得到较好的模型。个人经验是，聚类的时候高级特征尽量少一点，分类回归的时候高级特征适度的多一点。

16. 

###  <u>深度学习</u>

1. 梯度下降陷入局部最优有什么解决办法（2020TP-LINK） 
   * 使用模拟退化技术，模拟退火在每一步都以一定的概率接受比当前解更差的结果，从而有助于“跳出”局部极小。在每步迭代过程中，接受“次优解”的概率随着时间的推移而逐渐降低，从而保证算法的稳定。
   * 使用随机梯度下降，与标准的梯度下降法精确计算梯度不同，随机梯度下降法在计算梯度时加入了随机的因素。于是，即便陷入局部极小点，它计算出的梯度时加入了随机因素，于是，即便陷入局部极小点，它计算出的梯度可能不为0，这样就有机会跳出局部极小继续搜索。
   
2.  1 * 1卷积核的作用：

   （1）特征降维，减少计算量

   （2）增加模型非线性表达能力

3. word2vec的原理,还有哪些embedding模型

   其他的embedding模型：基于统计方法，使用共现矩阵，再进行SVD分解（U的行作为word embedding）Glove, ELMo(同一个词在不同语境下有不同的embedding)，GraphEmbedding的方法（node2vec, deepwalk, LINE等），GNN（GCN等，word embedding）

4. lstm相比于rnn解决了什么，为什么? lstm的门结构

   缓解了梯度爆炸和梯度消失。

5. 过拟合的解决办法

   增加数据量；加入正则项；Dropout; BN； 降低模型复杂度；减少特征数量

6. 数据不平衡怎么办，一般什么场合会出现数据不平衡

   对于多的样本进行欠采样，对于少的样本进行过采样；

   CTR预测中，用户点击的物品很少，未点击的物品很多

7. 说说GAN

8.  dropout的底层原理

   以一定的概率让神经元节点输出0，从而使其失效

9. embedding维度如何选取，维度不一样拼接在一起会怎么样？

10. 优化器的总结

    SGD：$ \theta_{t + 1} = \theta_t - \eta g_t$

    不足：山谷和鞍点两种地形，山谷导致收敛不稳定和收敛速度慢；鞍点导致梯度近乎为0，算法停滞。

    出发点：解决随机梯度下降法山谷震荡和鞍点停滞的问题，提出动量方法（惯性保持和环境感知）

    momentum:   $ v_t= \gamma v_{t -1} + \eta g_t,$

    ​                        $ \theta_{t + 1} = \theta_t - v_t$

    AdaGrad:采用“历史梯度平方和”来衡量不同参数的梯度的稀疏性，取值越小表明越稀疏。（自适应确定参数学习速率）

    $ \theta_{t+1, i} = \theta_{t,i} - \frac {\eta}{\sqrt {\sum_{k=0}^t g^2_{k, i} + \epsilon}}g_{t,i}$ 

    adam:将惯性保持和环境感知集于一身。一方面，记录梯度的一阶矩，即过往梯度与当前梯度的平均，体现惯性保持；另一方面，记录梯度的二阶矩，及过往梯度平方和当前梯度的平方的平均，类似adagrad,体现了环境感知。

    $ m_t = \beta_1m_{t-1}+(1-\beta_1)g_t,$

    $v_t = \beta_2 v_{t-1} + (1-\beta_2 g^2_t),$

    $\theta_{t+1} = \theta_t - \frac{\eta \cdot \hat {m_t}}{\sqrt {\hat v_t + \epsilon}}$

    其中，$\hat m_t = \frac{m_t}{1- \beta ^t_1}$

    ​          $\hat v_t = \frac{v_t}{1 - \beta^t _2}$

11. 优化算法

    - 最快下降法：负梯度方向，具有全局收敛性

      优点：算法每次迭代的计算量少，存储量也少，从一个不太好的初始点出发也能靠近极小点。

      缺点：收敛慢（线性收敛）；Zigzag现象（收敛慢原因）；没有二次终止性，即不具备对于任意的正定二次函数， 从任意点出发，都可以经过有限步迭代取得极小值的性质

    - Newton方法：

      - 基本的Newton方法

        设$f(x)$具有连续二阶偏导数，当前迭代点是$x_k$，$f(x)$在$x_k$的泰勒展开为：$f(x_k + d) = f_k + g^T_k d + \frac{1}{2}d^T G_k d + o(||d||^2)$

        其中$d = x - x_k$。在$x_k$的邻域内，用二次函数$q_k(d) = f_k + g^T_k d + \frac{1}{2}d^T G_k d$去近似$f(x_k + d)$，求解问题$min \quad q_k(d)=f_k+g^T_k d + \frac{1}{2} d^T G_kd$

        若$G_k$正定，则迭代方向$d_k = -G^{-1}_k g_k$为问题的唯一解，我们称$d_k = -G^{-1}_k g_k$为Newton方向。（Hesse逆矩阵度量下的最快下降法）

        结论：（1）当初始点接近极小点时，迭代序列收敛于极小点，并且收敛很快（二阶收敛）

        （2）当初始点不接近极小点时，迭代序列容易收敛到鞍点或者极大点（局部收敛性而不是全局收敛）。

        （3）迭代过程可能会出现奇异矩阵或者病态，以至于求逆很困难，导致迭代失败。

        - 当![img](https://mmbiz.qpic.cn/mmbiz_svg/3a3QxMHZ8YwnLDxI5vXUicU9GricZ5dGibAVojFWZ9peZ3YYsb2dLzM1MFyfxn5UXepKs0W9iapLG1zossdNpDTf3s1KgtFaRLxm/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)的特征值![img](https://mmbiz.qpic.cn/mmbiz_svg/3a3QxMHZ8YwnLDxI5vXUicU9GricZ5dGibAKs83hNjc4sibS5HaTEb206ls3R9xBkiaxaspkqwAEyFWUM9nvibLO2lEpgaIFdASORu/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)，![img](https://mmbiz.qpic.cn/mmbiz_svg/3a3QxMHZ8YwnLDxI5vXUicU9GricZ5dGibAJvKjKLYhmp6FgEZPV01SPYwhNPh3YyDFhgRicZG5N4iab9QDtxdgEjRectXA1EYHjZ/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)求不出来。

        - 当![img](https://mmbiz.qpic.cn/mmbiz_svg/3a3QxMHZ8YwnLDxI5vXUicU9GricZ5dGibAVojFWZ9peZ3YYsb2dLzM1MFyfxn5UXepKs0W9iapLG1zossdNpDTf3s1KgtFaRLxm/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)的特征值

          ![img](https://mmbiz.qpic.cn/mmbiz_svg/3a3QxMHZ8YwnLDxI5vXUicU9GricZ5dGibAibvfxAfCoNHxbNNicer13zhN2ER1WiaMw78XSJlE4YmlAfRvbYDaHCac597MD7iaqvBc/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1) 

          不一定小于0，牛顿方向未必是下降方向。

        （4）每一步迭代需要计算Hesse矩阵，即计算n(n+1)/2个二阶偏导数，相当于求解一个线性方程组，计算量为O(![img](https://mmbiz.qpic.cn/mmbiz_svg/3a3QxMHZ8YwnLDxI5vXUicU9GricZ5dGibA6KZ4zgzmzuVQhaGrwyC7005Mn0YGUZqHkQncroqKdP4QMbX0kZuxOwDGiciaicSngIib/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1))

      - 阻尼Newton方法

        为了改善基本Newton方法的局部收敛准则，我们采用带一维线搜索的的Newton方法，即

        ![img](https://mmbiz.qpic.cn/mmbiz_svg/3a3QxMHZ8YwnLDxI5vXUicU9GricZ5dGibATXVuPTQvbr4hLMNOkuMSASpd5Xt46FxiaBeoQB0F4Dm9TjQJosJiawGOf7GnUQrkLI/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

        其中![img](https://mmbiz.qpic.cn/mmbiz_svg/3a3QxMHZ8YwnLDxI5vXUicU9GricZ5dGibAOHBMO2SxhgKpLY4bSQoEEEAKV4Jz4DXTPgdHSClRblA91WOWic6jMAF9furTnRnKN/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)是一维搜索的结果，该方法叫做阻尼Newton方法。此方法能保证对正定矩阵![img](https://mmbiz.qpic.cn/mmbiz_svg/3a3QxMHZ8YwnLDxI5vXUicU9GricZ5dGibAz4Sm9rwpVmqqIoSibVibx0KgkbEibONhMicqXVicOsyQInW4m9Y00CRaJtdYd76YGLxj1/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)，![img](https://mmbiz.qpic.cn/mmbiz_svg/3a3QxMHZ8YwnLDxI5vXUicU9GricZ5dGibAOiaJXqCZuPTXicgaMqiaLXIibvggds95lMfkdqJmEkpXh5BGYrQzLx3icMPVdnyyCDN4b/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1) 单调下降；即使 ![img](https://mmbiz.qpic.cn/mmbiz_svg/3a3QxMHZ8YwnLDxI5vXUicU9GricZ5dGibAicEtVa2qkFrW4kQxL6jVSeVENbhQWZws8wNLU6dOqOl7icg6VYNtibvTZcp2z0b06dw/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1) 离x稍远，由该方法产生的点列![img](https://mmbiz.qpic.cn/mmbiz_svg/3a3QxMHZ8YwnLDxI5vXUicU9GricZ5dGibACpyzNObc7FYZwrDWcIYULFciaJsn7WiayibuvrWfqYyUW7UUFUULSNyO7wcOor4ut8c/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)仍能收敛到![img](https://mmbiz.qpic.cn/mmbiz_svg/3a3QxMHZ8YwnLDxI5vXUicU9GricZ5dGibADqBLAOD95NC4XL4JrNVSJUOQC6bV16bj2MFNHUSCFkdjdolU9JXHibyVZO9lfChS6/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)。（对严格凸函数具有全局收敛性）

      - 混合方法

        基本Newton方法在迭代过程中会出现Hesse矩阵奇异、不正定的情形，基本Newton方法还会出现与![img](https://mmbiz.qpic.cn/mmbiz_svg/3a3QxMHZ8YwnLDxI5vXUicU9GricZ5dGibA7XXzdVmRPv7Pn2LsshAvtWcW0RoY9iak39J6fPOVgdZzfXhMFbIyiaOpDeZgxGibJSO/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)几乎正交的情形。为了解决这个问题，我们可以采用基本Newton方法与最速下降法相互混合的方式。

        该方法采用Newton方法，但是在Hesse矩阵![img](https://mmbiz.qpic.cn/mmbiz_svg/3a3QxMHZ8YwnLDxI5vXUicU9GricZ5dGibAz4Sm9rwpVmqqIoSibVibx0KgkbEibONhMicqXVicOsyQInW4m9Y00CRaJtdYd76YGLxj1/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)奇异或者![img](https://mmbiz.qpic.cn/mmbiz_svg/3a3QxMHZ8YwnLDxI5vXUicU9GricZ5dGibA7XXzdVmRPv7Pn2LsshAvtWcW0RoY9iak39J6fPOVgdZzfXhMFbIyiaOpDeZgxGibJSO/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)与![img](https://mmbiz.qpic.cn/mmbiz_svg/3a3QxMHZ8YwnLDxI5vXUicU9GricZ5dGibA3YIX7Oehj4ZsQ3QjiapdmucCUMce7nl2ibvK9dHfLuluJdTdIaulrTupISgdUkicC65/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)几乎正交时，采用负梯度方向；在![img](https://mmbiz.qpic.cn/mmbiz_svg/3a3QxMHZ8YwnLDxI5vXUicU9GricZ5dGibAz4Sm9rwpVmqqIoSibVibx0KgkbEibONhMicqXVicOsyQInW4m9Y00CRaJtdYd76YGLxj1/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)负定，但是![img](https://mmbiz.qpic.cn/mmbiz_svg/3a3QxMHZ8YwnLDxI5vXUicU9GricZ5dGibArUibNquTO0IqqKgJYxC8uqTjkIwicibu0CffJ3sTriaocrMLYhjeP9VEoQ8Du5GhVY8t/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)存在时，取![img](https://mmbiz.qpic.cn/mmbiz_svg/3a3QxMHZ8YwnLDxI5vXUicU9GricZ5dGibAmvn1pyq0ricdgCoB5VibEuQxjDWIvU1RzLroSJKiaiaMFmPLzdrKj4Ac1w24BIux2aSC/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1) 。

      - LM方法

        LM方法是处理![img](https://mmbiz.qpic.cn/mmbiz_svg/3a3QxMHZ8YwnLDxI5vXUicU9GricZ5dGibAz4Sm9rwpVmqqIoSibVibx0KgkbEibONhMicqXVicOsyQInW4m9Y00CRaJtdYd76YGLxj1/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)奇异、不正定等情况的一个最简单有效的方法，它是指求解 ![img](https://mmbiz.qpic.cn/mmbiz_svg/3a3QxMHZ8YwnLDxI5vXUicU9GricZ5dGibAVJzG8QkVTL2hcg9PWic3315SPRQiaibPPoJeF1WvibJBLFAx5ZlWWMWLRnsEy66PRTia4/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)来确定迭代方向的Newton型方法，这里的![img](https://mmbiz.qpic.cn/mmbiz_svg/3a3QxMHZ8YwnLDxI5vXUicU9GricZ5dGibAlZNCv1bPDvrnKVpibxjxUcZYXlgpmWj7AwKrEdbGMeYRwahddphMZicAUTyRB7OoXq/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)是单位阵。显然，若![img](https://mmbiz.qpic.cn/mmbiz_svg/3a3QxMHZ8YwnLDxI5vXUicU9GricZ5dGibAETAuWwxEqssRHett0B58AzteDhKtAz1mxAWk7u8AQ9t1HSRlPg9A7tOqEswkEC6Y/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)足够大，可以保证![img](https://mmbiz.qpic.cn/mmbiz_svg/3a3QxMHZ8YwnLDxI5vXUicU9GricZ5dGibAFyia4zISzZDbu2DBtDmQXTCtfq7KOxhxwIp6Azxd8BeSRLZjYS0QRTxewvVeGQP1W/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)正定。

      - 拟牛顿法

        Newton方法的优缺点：

        （1）当初始点接近极小点时，迭代序列收敛于极小点，并且收敛很快（二阶收敛）；

        （2）当初始点不接近极小点时，迭代序列容易收敛到鞍点或者极大点（局部收敛性而不是全局收敛）。

        （3）迭代过程可能会出现奇异矩阵或者病态，以至于求逆很困难，导致迭代失败。

        - 当![img](https://mmbiz.qpic.cn/mmbiz_svg/3a3QxMHZ8YwnLDxI5vXUicU9GricZ5dGibAVojFWZ9peZ3YYsb2dLzM1MFyfxn5UXepKs0W9iapLG1zossdNpDTf3s1KgtFaRLxm/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)的特征值![img](https://mmbiz.qpic.cn/mmbiz_svg/3a3QxMHZ8YwnLDxI5vXUicU9GricZ5dGibAKs83hNjc4sibS5HaTEb206ls3R9xBkiaxaspkqwAEyFWUM9nvibLO2lEpgaIFdASORu/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)，![img](https://mmbiz.qpic.cn/mmbiz_svg/3a3QxMHZ8YwnLDxI5vXUicU9GricZ5dGibAJvKjKLYhmp6FgEZPV01SPYwhNPh3YyDFhgRicZG5N4iab9QDtxdgEjRectXA1EYHjZ/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)求不出来。

        - 当![img](https://mmbiz.qpic.cn/mmbiz_svg/3a3QxMHZ8YwnLDxI5vXUicU9GricZ5dGibAVojFWZ9peZ3YYsb2dLzM1MFyfxn5UXepKs0W9iapLG1zossdNpDTf3s1KgtFaRLxm/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)的特征值![img](https://mmbiz.qpic.cn/mmbiz_svg/3a3QxMHZ8YwnLDxI5vXUicU9GricZ5dGibADoLeTFKWGCr63YC4ajeOAnjk81h2X8ianczUPQ5tRhHYdgicZiaL6rnQglJN8K7OqfQ/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1), 

          ![img](https://mmbiz.qpic.cn/mmbiz_svg/3a3QxMHZ8YwnLDxI5vXUicU9GricZ5dGibAjUI5g3HYzNBv1EQ5JxtM07vvtMMhOVG6pG45uzJ5xUdaTdibqN0pLfKfK2lxia4US4/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

          不一定小于0，牛顿方向未必是下降方向。

        （4）每一步迭代需要计算Hesse矩阵，即计算n(n+1)/2个二阶偏导数，相当于求解一个线性方程组，计算量为O(![img](https://mmbiz.qpic.cn/mmbiz_svg/3a3QxMHZ8YwnLDxI5vXUicU9GricZ5dGibA6KZ4zgzmzuVQhaGrwyC7005Mn0YGUZqHkQncroqKdP4QMbX0kZuxOwDGiciaicSngIib/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1))

        为此，我们考虑构造一种方法，她既不需要计算二阶偏导数，又有较快的收敛速度。

        假定当前迭代点为![img](https://mmbiz.qpic.cn/mmbiz_svg/3a3QxMHZ8YwnLDxI5vXUicU9GricZ5dGibAsAxb36JoB0JcdafTP7xcMY9AMIic0E6EtOdgbR0DL1mNv44VoGkO0aibwWpXzupicq6/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)，已知条件为![img](https://mmbiz.qpic.cn/mmbiz_svg/3a3QxMHZ8YwnLDxI5vXUicU9GricZ5dGibAhQhOdkht0RiaQvD6LgljLuCLE9VMajSr0vUZKtbEOH7lhx4vZC0vZib2WbIdUmHnTP/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)，我们使用拉格朗日中值定理： 

        ![img](https://mmbiz.qpic.cn/mmbiz_svg/3a3QxMHZ8YwnLDxI5vXUicU9GricZ5dGibA99ulr1h8N3GRBWhEW0Hic6ubVdhzcyO544rOX2bc7gu1TxD2SqicobREsUHMfU6hoX/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

        我们可以使用矩阵![img](https://mmbiz.qpic.cn/mmbiz_svg/3a3QxMHZ8YwnLDxI5vXUicU9GricZ5dGibAszntLoEIvuCdwfjFkepJYKlg1ZKGbwGT5FdzPvgqibEoxJKyhAV5Idr4IeJbjC4Pf/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)似![img](https://mmbiz.qpic.cn/mmbiz_svg/3a3QxMHZ8YwnLDxI5vXUicU9GricZ5dGibAl4QqLfib5VnWibUFR0FbQU03jpt9GoJRAnic4ZySSD7qhqD3oZrKx0Daa6mVkINX2qy/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)得到 ![img](https://mmbiz.qpic.cn/mmbiz_svg/3a3QxMHZ8YwnLDxI5vXUicU9GricZ5dGibAe7O6ibHeXGWIyq6M2OXibwMe365kBraIsThu45iaOl86gdsj7Lia0enTnvB6aoZC39NL/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1) n个方程，n(n+1)/2个变量。

        令![img](https://mmbiz.qpic.cn/mmbiz_svg/3a3QxMHZ8YwnLDxI5vXUicU9GricZ5dGibABDkeXqts71XicK1T8TMIRHRhdZFWBJYs7RNgXtNrF9eU8xkICk3U0zl2ckibujicrrY/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)得到：

         ![img](https://mmbiz.qpic.cn/mmbiz_svg/3a3QxMHZ8YwnLDxI5vXUicU9GricZ5dGibAsW2cicaicsS6I5IV5eu3pykprtXicSicC2ZwLuZvXR1ynB6e3WAchPMJGJ2nt5lhwicf9/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

        因此拟牛顿条件为：

         ![img](https://mmbiz.qpic.cn/mmbiz_svg/3a3QxMHZ8YwnLDxI5vXUicU9GricZ5dGibAsW2cicaicsS6I5IV5eu3pykprtXicSicC2ZwLuZvXR1ynB6e3WAchPMJGJ2nt5lhwicf9/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1) 

        满足这两个方程的矩阵有很多，因此拟牛顿方法是一类方法。

      - 共轭梯度

        

12. 





### <u>推荐算法</u>

1. FM是否能起到自动特征选择的作用，为什么。（2020字节跳动）

2. deepFM和wide&deep有什么区别？wide&deep是什么样的结构？（补充：你在使用deepFM的时候是如何处理欠拟合和过拟合问题的？）（2020VIVO)

3.  传统的机器学习 和 深度学习在推荐上的异同点，深度学习的优势在哪里?

   |      | 异                             | 同   |
   | ---- | ------------------------------ | ---- |
   | ML   | 需要人工筛选特征，缺少高阶特征 |      |
   | DL   |                                |      |

   DL优势：自动选择特，产生高阶特征。

   

4.  FM隐向量思路来源：Poly2, 具体做法；

   化简：直接计算O(kn^2)化简成O(kn)

   参数更新：梯度下降

   从FM->FFM:引入了field的概念，核心目标在于，很多时候没有必要衡量任意两个小特征的关系，而只需要衡量小特征和每个field之间的关系，这样能一定程度降低稀疏性，提升隐向量的实际含义和泛化能力。

5.  DeepFM和NFM架构的区别（并&串）

6. fm，ffm模型原理
   fm和ffm谁的参数多，域的划分

7. 推荐中为什么对负样本进行下采样，关于采样率在loss中的体现，之后的一系列公式手推

8. DNN，FM能做一阶特征吗，如果可以那为什么还用LR

   DNN是高阶， FM还包含二阶特征，LR只有一阶特征

9. 从推荐算法角度，怎么提高价格低的商品的曝光量

10. 冷启动问题。

11. 怎么判断小视频是真的火。有长视频，有短视频，如果长视频内容不吸引人注意，用户看了十几秒就退出，即使长视频的播放时间比短视频长也不一定证明长视频的内容更吸引用户的注意，怎么处理这种问题？

12.  推荐中的排序是为了干什么，有什么方法。这些机器学习算法在召回阶段可以用吗，目的是什么？为什么wide and deep的Wide部分还用lr？







### <u>图算法</u>





### <u>数学题</u>

1. 两个随机变量X,Y服从独立同分布，均匀分布U(0,1),求max(X,Y)的期望

   令Z = max{X, Y}, 从定义出发，求出Z的概率分布函数，进而得到Z的概率密度函数，然后根据连续随机变量的期望计算公式进行计算，答案是2/3.

2. 一个圆上三点能构成锐角三角形概率

3. 54张扑克牌分成三份儿平均。大小王在同一堆的概率

   1/3

4. 